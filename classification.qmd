# Classification {#sec-classification}
A machine learning classifier learns patterns from categorized data and then predicts the category of new, unseen data. This module provides a set of machine learning classifiers and interactive widgets to help users perform classification. 

All the classifiers are implemented using `scikit-learn` and the performance metrics are calculated using `sklearn.metrics`. A fixed random seed (42) is used for all the classifiers to ensure reproducibility. 

![](analysis_ui_shots/classification.png){width=100% fig-align=center}
The performance metric plots are cut off in the screenshot. See the [performance metrics section](#performance-metrics) for the complete view. 

## Shared Interface Components
- On the left, users can use the [selection widgets](data_analysis.qmd#multivariate-analysis) to select numerical features for the classifier. Multiple features can be selected from each feature group. 
- On the top right, users can use the [filters](data_analysis.qmd#filter-widgets) to subset the data to find the groups of interest. 
- Above the performance metrics plots, users can change the plot style using the [plot styling widgets](data_analysis.qmd#plotting-configuration-widgets). 

## Method-specific Components
- Choose the classifier and train-test split: the slider controls the proportion of the data used to train the classifier and the rest is used as unseen data to test its generalizability.

![](analysis_ui_shots/classifier_config.png){width=60% fig-align=center}

- `Classify by`: The `Color by` from the [visual channels widgets](data_analysis.qmd#visual-channels-widgets) is renamed to `Classify by` and it is used to form classes based on (a combination of) selected categorical features. Other visual channels are not supported in this method. 

![](analysis_ui_shots/classify_by.png){width=90% fig-align=center}

- Choose the classes to be classified: classes are formed based on the combinations of categories of the selected categorical features. In general, it supports binary classification and in general n-way classification where n is the number of classes. If one categorical feature is selected and after filtering has 3 categories, then all possible ways to classify are produced including the three-way classification. Additionally, the `class x vs the rest` option is provided for each class. 

![](analysis_ui_shots/classifier_options.png){width=30% fig-align=center}

- Sampling methods: 
    - `None`: stratified random sampling of the data on classification classes, regardless of the class size. It is implemented using `train_test_split` from `sklearn.model_selection`. 
    - `Undersampling`: All classes are downsampled to the size of the smallest class. It is implemented using `RandomUnderSampler` from `imbalanced-learn`. 
    - `Oversampling`: All classes are upsampled to the size of the largest class by randomly duplicating samples from minority classes. It is implemented using `RandomOverSampler` from `imbalanced-learn`. 

![](analysis_ui_shots/sampling.png){width=20% fig-align=center}

::: {.callout-note}
FLIM Playground applies the z-score normalization to the features before training if the chosen classifier is `SVM` or `Logistic Regression`, because they are sensitive to feature magnitudes, while `Random Forest` and `Gradient Boosting` are not. 
:::

### Performance Metrics
To evaluate the performance of the classifiers, performance metrics visualizations are provided. 

#### Confusion Matrix
FLIM Playground evaluates the trained classifier on the held-out test set and summarizes its predictions with `sklearn.metrics.confusion_matrix`. In the binary case, it is represented as: 

|                     | Predicted Positive                     | Predicted Negative                          |
| ------------------- | -------------------------------------- | ------------------------------------------- |
| **Actual Positive** | **True Positive (TP)** – correct hits  | **False Negative (FN)** – missed positives  |
| **Actual Negative** | **False Positive (FP)** – false alarms | **True Negative (TN)** – correct rejections |

Other metrics can be calculated based on the confusion matrix:

- Accuracy: $\frac{TP + TN}{TP + FP + TN + FN}$
- Precision: $\frac{TP}{TP + FP}$
- Recall (Sensitivity): $\frac{TP}{TP + FN}$
- F1 Score: $\frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$

![](analysis_ui_shots/classifier_performance.png){width=90% fig-align=center}

#### ROC Curve
Another way to look at the performance of the classifier is to plot the Receiver Operating Characteristic (ROC) curve and its Area Under the Curve (AUC). 

A classifier usually outputs a score for each sample. To turn scores into hard labels, a decision threshold must be chosen. Confusion matrix looks at the performance of the classifier at a single threshold (in the binary case, the threshold is 0.5), while ROC curve looks at the performance of the classifier at all thresholds (from 0 to 1). 

At the origin (0, 0), the threshold is 1, so the classifier predicts all samples as negative, making the false positive rate $\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}} = 0$ and the true positive rate $\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}} = 0$, i.e. the origin. At the point (1, 1), the threshold is 0, so the classifier predicts all samples as positive, making the false positive rate 1 and the true positive rate 1, i.e. the point (1, 1). 

AUC summarizes the ROC curve by integrating it from 0 to 1, so it is threshold-free and has a nice interpretation: it is the probability that the classifier assigns a higher score to a randomly chosen positive sample $X^+$ than to a randomly chosen negative sample $X^-$. 

To generalize to the $K$-class cases ($K > 2$), FLIM Playground chooses the One-vs-Rest (OvR) strategy. For each class $i$, it trains a binary classifier to predict the class $i$ against all other classes. The ROC curve is then computed and plotted for each class. 

![](analysis_ui_shots/threeway_classifier.png){width=90% fig-align=center}


#### Feature Importance
`Random Forest` and `Gradient Boosting` classifiers provide feature importance scores. 

![](analysis_ui_shots/feature_importance.png){width=90% fig-align=center}
